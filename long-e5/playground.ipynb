{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunyu/long-e5/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mosaic_bert_transformer import MosaicBertTransformer\n",
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 845/845 [00:00<00:00, 5.49MB/s]\n",
      "Downloading (…)onfiguration_bert.py: 100%|██████████| 1.01k/1.01k [00:00<00:00, 5.99MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mosaic-bert-base-seqlen-2048:\n",
      "- configuration_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)/main/bert_layers.py: 100%|██████████| 47.3k/47.3k [00:00<00:00, 149MB/s]\n",
      "Downloading (…)flash_attn_triton.py: 100%|██████████| 42.7k/42.7k [00:00<00:00, 92.6MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mosaic-bert-base-seqlen-2048:\n",
      "- flash_attn_triton.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)main/bert_padding.py: 100%|██████████| 6.26k/6.26k [00:00<00:00, 26.0MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mosaic-bert-base-seqlen-2048:\n",
      "- bert_padding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mosaic-bert-base-seqlen-2048:\n",
      "- bert_layers.py\n",
      "- flash_attn_triton.py\n",
      "- bert_padding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 550M/550M [00:07<00:00, 72.9MB/s] \n",
      "Some weights of the model checkpoint at mosaicml/mosaic-bert-base-seqlen-2048 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 169kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 2.81MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 334MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 232MB/s]\n"
     ]
    }
   ],
   "source": [
    "bert_model = MosaicBertTransformer(\"mosaicml/mosaic-bert-base-seqlen-2048\", max_seq_length=2048)\n",
    "pooling_model = models.Pooling(bert_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[bert_model, pooling_model], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "# Monkeypatch scheduler\n",
    "def _patched_get_scheduler(optimizer, scheduler: str, warmup_steps: int, t_total: int):\n",
    "    if isinstance(optimizer, Adafactor):\n",
    "        return AdafactorSchedule(optimizer)\n",
    "    else:\n",
    "        return SentenceTransformer._get_scheduler(optimizer, scheduler, warmup_steps, t_total)\n",
    "\n",
    "model._get_scheduler = staticmethod(_patched_get_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1154,  0.6300,  0.3592,  ..., -0.0801, -0.3798,  0.3697],\n",
       "        [-0.1958,  0.5148,  0.3999,  ..., -0.3005, -0.5117,  0.4117],\n",
       "        [-0.1179,  0.3855,  0.7096,  ...,  0.2644, -0.4451,  0.6874]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"Hello world\", \"How are you?\", \"I am fine.\"]\n",
    "model.encode(texts, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
